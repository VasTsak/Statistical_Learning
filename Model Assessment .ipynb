{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The generalization performance of a learning method relates to its prediction\n",
    "capability on independent test data. Assessment of this performance\n",
    "is extremely important in practice, since it guides the choice of learning\n",
    "method or model, and gives us a measure of the quality of the ultimately\n",
    "chosen model\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "\n",
    "## Validation Set Approach\n",
    "\n",
    "It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate provides an estimate of the test error rate. \n",
    "\n",
    "<img src=\"validation_set_approach.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>5.471939</td>\n",
       "      <td>194.411990</td>\n",
       "      <td>104.469388</td>\n",
       "      <td>2977.584184</td>\n",
       "      <td>15.541327</td>\n",
       "      <td>75.979592</td>\n",
       "      <td>1.576531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.805007</td>\n",
       "      <td>1.705783</td>\n",
       "      <td>104.644004</td>\n",
       "      <td>38.491160</td>\n",
       "      <td>849.402560</td>\n",
       "      <td>2.758864</td>\n",
       "      <td>3.683737</td>\n",
       "      <td>0.805518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1613.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2225.250000</td>\n",
       "      <td>13.775000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>2803.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>275.750000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>3614.750000</td>\n",
       "      <td>17.025000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>5140.000000</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mpg   cylinders  displacement  horsepower       weight  \\\n",
       "count  392.000000  392.000000    392.000000  392.000000   392.000000   \n",
       "mean    23.445918    5.471939    194.411990  104.469388  2977.584184   \n",
       "std      7.805007    1.705783    104.644004   38.491160   849.402560   \n",
       "min      9.000000    3.000000     68.000000   46.000000  1613.000000   \n",
       "25%     17.000000    4.000000    105.000000   75.000000  2225.250000   \n",
       "50%     22.750000    4.000000    151.000000   93.500000  2803.500000   \n",
       "75%     29.000000    8.000000    275.750000  126.000000  3614.750000   \n",
       "max     46.600000    8.000000    455.000000  230.000000  5140.000000   \n",
       "\n",
       "       acceleration       model      origin  \n",
       "count    392.000000  392.000000  392.000000  \n",
       "mean      15.541327   75.979592    1.576531  \n",
       "std        2.758864    3.683737    0.805518  \n",
       "min        8.000000   70.000000    1.000000  \n",
       "25%       13.775000   73.000000    1.000000  \n",
       "50%       15.500000   76.000000    1.000000  \n",
       "75%       17.025000   79.000000    2.000000  \n",
       "max       24.800000   82.000000    3.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original\",\n",
    "                   delim_whitespace = True, header=None,\n",
    "                   names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration',\n",
    "                            'model', 'origin', 'car_name'])\n",
    "data = data.dropna(axis = 0)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25.57387818968439"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['horsepower'].reshape(-1, 1) # We need to reshape the array so that we can feed it to the model\n",
    "y = data['mpg']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Simple linear regression\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_pred = lm.predict(x_test)\n",
    "\n",
    "np.mean((y_test - lm_pred)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.218020050032855"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try quadratic regression\n",
    "X_2 = np.hstack((X, X**2)) \n",
    "x_train, x_test, y_train, y_test = train_test_split(X_2, y, test_size=0.5, random_state=42)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_pred = lm.predict(x_test)\n",
    "np.mean((y_test - lm_pred)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.667675435534484"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try cubic regression\n",
    "X_3 = np.hstack((X, X**2, X**3))\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_3, y, test_size=0.5, random_state=42)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_pred = lm.predict(x_test)\n",
    "np.mean((y_test - lm_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the *random state* value to see if we observe any difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.41900883210308\n",
      "20.19623577421668\n",
      "20.26642632189447\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=41)\n",
    "# Simple linear regression\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_pred = lm.predict(x_test)\n",
    "\n",
    "print(np.mean((y_test - lm_pred)**2))\n",
    "\n",
    "# Quadratic regression\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_2, y, test_size=0.5, random_state=41)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_pred = lm.predict(x_test)\n",
    "\n",
    "print(np.mean((y_test - lm_pred)**2))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_3, y, test_size=0.5, random_state=41)\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_pred = lm.predict(x_test)\n",
    "\n",
    "print(np.mean((y_test - lm_pred)**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that non linear methods perform much better. But also we notice that this way of validating is really not consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation Approach\n",
    "\n",
    "It is very similar to **Validation Set Approach** but it attempts to address this method's drawbacks. \n",
    "\n",
    "It is also separates the whole data set into training and validation set, but instead of creating two data sets of comparing size a single observation is used for validation set. This process takes place up *n* times so that we have used all the observations as a validation set. \n",
    "\n",
    "\n",
    "<img src=\"loocv.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The estimate of test error from this method is quite poor because it will be quite variable since it is based on only one observation.\n",
    "\n",
    "Let's assume that we use as a loss function the MSE. The LOOCV estimate for test error is: \n",
    "$$CV = \\dfrac{1}{n}\\sum_{i}^{n}MSE_{i} $$ \n",
    "\n",
    "Why do we use LOOCV approach over the Validation Set approach?\n",
    "\n",
    "* Less bias, the LOOCV approach doesn't overestimate the test set as Validation Set approach does.\n",
    "* Validation set approach will return different results if you run it multiple times due to the randomness of set selection but LOOCV will return the same results\n",
    "\n",
    "LOOCV can be very expensive if we have a big data set. Is there another cheaper way to do that? Of course there is!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 24.2315135179 The standard deviation is: 36.7973150364\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "error = []\n",
    "lm = linear_model.LinearRegression()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 19.2482131245 The standard deviation is: 34.9984461518\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "error = []\n",
    "lm = linear_model.LinearRegression()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    x_train, x_test = X_2[train_index], X_2[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 19.334984064 The standard deviation is: 35.7651356779\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "error = []\n",
    "lm = linear_model.LinearRegression()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    x_train, x_test = X_3[train_index], X_3[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the estimation of the error is much more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Fold Cross Validation approach\n",
    "\n",
    "This approach divides the original set into $k$ groups(aka folds) of equal size. The first fold is treated as validation set and the method is fitted into the rest $k-1$ folds. This process, results into $k$ estimates.\n",
    "\n",
    "<img src=\"kfolds.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Let's assume that we use as a loss function the MSE. The $k$-Fold CV estimate for test error is: \n",
    "\n",
    "$$CV = \\dfrac{1}{k}\\sum_{i}^{k}MSE_{i}$$  \n",
    "\n",
    "Why do we use k-Fold CV instead of LOOCV?\n",
    "\n",
    "* It is much faster\n",
    "* Bias-Variance trade-off, $k$-Fold CV gives better test estimates because:\n",
    "    * Validation set approach overestimates the test error and LOOCV is quite unbiased while $k$-Fold is in between. When we consider to minimize bias we go for LOOCV, but bias is not the only source for concern of estimation;\n",
    "    * LOOCV has higher variance than $k$-Fold CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 27.4399336523 The standard deviation is: 14.5102507113\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits = 10)\n",
    "error = []\n",
    "lm = linear_model.LinearRegression()\n",
    "for train_index, test_index in k_fold.split(X):\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 21.2358400558 The standard deviation is: 11.7973275289\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits = 10)\n",
    "error = [] \n",
    "lm = linear_model.LinearRegression()\n",
    "for train_index, test_index in k_fold.split(X_2):\n",
    "    x_train, x_test = X_2[train_index], X_2[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 21.3366061832 The standard deviation is: 11.8443397146\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits = 10)\n",
    "error = []\n",
    "lm = linear_model.LinearRegression()\n",
    "for train_index, test_index in k_fold.split(X_3):\n",
    "    x_train, x_test = X_3[train_index], X_3[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that K-Fold is much faster approach. Also notice the variance of the two methods. K-fold has a less variant result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap\n",
    "\n",
    "Bootstrap is a method used to quantify the uncertainty of the estimator (e.g standard errors of the coefficients in linear regression) of a statistical learning model. \n",
    "\n",
    "\n",
    "It samples randomly with replacement *m* data sets with *n* observations from the original data set. Having created the *m* data sets\n",
    "\n",
    "<img src=\"bootstrapping.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Having done that, you calculate the variance for every sample data set and then average it.\n",
    "\n",
    "$$S(\\bar{X}) = \\dfrac{1}{m}\\sum_{i=1}^{m}(S(X^{*i})) $$\n",
    "\n",
    "The function for calculating the bootstrap method was found [here](http://nbviewer.jupyter.org/gist/aflaxman/6871948)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error is: 24.1620693417 The standard deviation is: 2.90864286876\n",
      "The average error is: 21.7092268932 The standard deviation is: 3.7026353213\n",
      "The average error is: 20.9242564749 The standard deviation is: 3.54980821034\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_resample(X, n=None):\n",
    "    \"\"\" Bootstrap resample an array_like\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "      data to resample\n",
    "    n : int, optional\n",
    "      length of resampled array, equal to len(X) if n==None\n",
    "    Results\n",
    "    -------\n",
    "    returns X_resamples\n",
    "    \"\"\"\n",
    "    if n == None:\n",
    "        n = len(X)\n",
    "        \n",
    "    resample_i = np.floor(np.random.rand(n)*len(X)).astype(int)\n",
    "    X_resample = X[resample_i]\n",
    "    return X_resample\n",
    "\n",
    "for i in range(1000):\n",
    "    train_index = bootstrap_resample(np.where(X)[0], n=196)\n",
    "    test_index = bootstrap_resample(np.where(X)[0], n=196)\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))\n",
    "\n",
    "for i in range(1000):\n",
    "    train_index = bootstrap_resample(np.where(X)[0], n=196)\n",
    "    test_index = bootstrap_resample(np.where(X)[0], n=196)\n",
    "    x_train, x_test = X_2[train_index], X_2[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))\n",
    "\n",
    "for i in range(1000):\n",
    "    train_index = bootstrap_resample(np.where(X)[0], n=196)\n",
    "    test_index = bootstrap_resample(np.where(X)[0], n=196)\n",
    "    x_train, x_test = X_3[train_index], X_3[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lm.fit(x_train, y_train)\n",
    "    lm_pred = lm.predict(x_test)\n",
    "    error.append((np.mean((y_test - lm_pred)**2)))\n",
    "\n",
    "print(\"The average error is:\",np.mean(error), \"The standard deviation is:\",np.std(error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
