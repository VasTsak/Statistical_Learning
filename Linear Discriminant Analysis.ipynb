{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overview \n",
    "\n",
    "**Linear Discriminant Analysis** is a very good alternative of Logistic regression. \n",
    "\n",
    "In general, It is a less direct approach of predicting the target. It models the distribution of predictors for every Y ($P(X|Y)$ and then it uses Bayes' theorem to calculate the $P(Y|X)$. When we assume that the distributions of $P(X|Y)$ are normal, the model is very similar to logistic regression. \n",
    "\n",
    "But when do we prefer LDA over Logistic regression?\n",
    "\n",
    "* When the classes are well-separated Logistic regression tends to be quite unstable, LDA not so much.\n",
    "* When the number of observations is small and the distribution of X for every class is normal, LDA tends to be more stable\n",
    "* LDA is preferred more when we have a multi-class problem.\n",
    "\n",
    "The Bayes theorem formula is: \n",
    "\n",
    "$$P(Y = k |X = x) = \\dfrac{P(X = x|Y = k)P(Y = k)}{P(X = x)} $$\n",
    "$$P(Y = k |X = x) = \\dfrac{P(X = x|Y = k)P(Y = k)}{\\sum_{l = 1}^{classes} P(X = x | Y = l)P(Y = l)} $$\n",
    "\n",
    "\n",
    "Where:\n",
    "* $P(X = x)$ is the prior probability that a random observation comes from class k.\n",
    "* $P(X = x|Y = k)$ is the density function of X for an observation that comes from the $k^{th}$ class\n",
    "* $P(Y = k |X = x)$ is the posterior probability that an observation x belongs to class k.\n",
    "\n",
    "At the end of the day, LDA is a classifier that approximates Bayes classifier.\n",
    "\n",
    "It gets really mathy, and I will not go through it. In case you are interested into the math(it is totally recommended to check it out) go for two classes [here](https://en.wikipedia.org/wiki/Linear_discriminant_analysis#LDA_for_two_classes) and for a multi-class model [here](https://en.wikipedia.org/wiki/Linear_discriminant_analysis#Multiclass_LDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf_linear = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "clf_linear.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Discriminant Analysis methods \n",
    "\n",
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "In this method we omit an assumption that I am not discussing here because you need to study the links I have provided before in order to understand. We use it when data are not linearly separable. \n",
    "\n",
    "But to find how it differs go [here](https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97222222222222221"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_quant = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\n",
    "clf_quant.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes method, is quite a simplistic approach. Because it takes the assumption that all the predictors are independent in each class. It is useful when the number of classes it too large and even QDA and LDA break down. It is widely used at sentiment classification tasks. \n",
    "\n",
    "While its wild assumption, it performs very well.\n",
    "\n",
    "The formula for Naive Bayes Classifier is:\n",
    "$$ P(Y | x_1, x_2, x_3 ...x_n)  \\propto P(Y) \\prod_{i=n}^{n} P(x_{i}|Y)$$ ince $P(x_1, \\dots, x_n)$ is constant given the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6b2591d406be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclf_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "clf_nb = gnb.fit(X_train, y_train)\n",
    "clf_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
